{"tokenizer": {"model_name": "/home/xyq/SS-RLHF/output", "padding_side": "left", "truncation_side": "left", "pad_token_as_eos_token": true}, "reward_fn": {"id": "intent_accuracy", "args": {"intent_coeff": 0.5, "auto_coeff": 0.5}}, "datapool": {"id": "strategyQA", "args": {"prefix": "Provide proof and reasons to answer this question: ", "suffix": " Your answer must be from the following options: "}}, "env": {"n_envs": 1, "args": {"max_prompt_length": 128, "max_episode_length": 100, "terminate_on_eos": true}}, "alg": {"id": "ppo", "args": {"n_steps": 128, "batch_size": 2, "verbose": 1, "learning_rate": 1e-06, "n_epochs": 5}, "kl_div": {"coeff": 0.2, "target_kl": 0.05}, "policy": {"id": "causal_lm_actor_critic_policy", "args": {"model_name": "/home/xyq/SS-RLHF/output", "apply_model_parallel": true, "generation_kwargs": {"do_sample": true, "top_k": 20, "min_length": 2, "max_new_tokens": 30}}}}, "train_evaluation": {"eval_batch_size": 2, "n_iters": 50, "eval_every": 5, "save_every": 10, "metrics": [{"id": "intent_accuracy"}, {"id": "causal_perplexity", "args": {"tokenizer_id": "/home/xyq/SS-RLHF/output", "stride": 128, "model_type": "causal"}}, {"id": "diversity", "args": {}}, {"id": "meteor", "args": {}}, {"id": "rouge"}, {"id": "bleu", "args": {}}, {"id": "bert_score", "args": {"language": "en"}}, {"id": "sacre_bleu", "args": {"tokenize": "intl"}}], "generation_kwargs": {"do_sample": true, "top_k": 20, "min_length": 2, "max_new_tokens": 250}}}